# LLM Switching Guide

This guide explains the LLM selection options in the Streamlit UI.

## ğŸ¤– LLM Options

### 1. ğŸ”„ Auto Mode (Recommended)
- How it works: Uses OpenAI if internet is available, otherwise Local LLM
- Advantages: Automatic switching, best overall performance
- Usage: Default choice

### 2. ğŸ”‘ OpenAI Only
- How it works: Uses only OpenAI API
- Advantages: Highest quality, fast
- Requirements: Internet + OpenAI API key

### 3. ğŸ  Local Only
- How it works: Uses only Local LLM
- Advantages: Works offline, data stays local
- Requirements: Sufficient RAM (8GB+)

## ğŸ¯ Usage Scenarios

### Scenario 1: Normal Use
```
Choice: Auto Mode
State: Internet available + OpenAI key present
Result: Uses OpenAI
```

### Scenario 2: Internet Outage
```
Choice: Auto Mode
State: No internet
Result: Uses Local LLM
```

### Scenario 3: No OpenAI Key
```
Choice: Auto Mode
State: Internet available but no OpenAI key
Result: Uses Local LLM
```

### Scenario 4: Fully Offline
```
Choice: Local Only
State: No internet
Result: Uses Local LLM
```

## ğŸ”§ Configuration

### OpenAI API Key
```bash
# Create .env
echo "OPENAI_API_KEY=your_api_key_here" > .env
```

### Local LLM Model (Ollama)
```env
# .env (recommended)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=tinyllama   # default in this project
# Alternatives (if pulled): gemma3:4b, llama3:8b, etc.
```

```python
# config.py reads from environment
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "tinyllama")
```

## ğŸ“Š Status Indicators

### Sidebar shows:
- ğŸŒ **Internet**: Connectivity status
- ğŸ”‘ **OpenAI**: API availability
- ğŸ¯ **Currently using**: Active LLM

### Metadata includes:
- **Selected Mode**: User-selected mode
- **Effective Mode**: Actually used mode
- **Token Usage**: LLM usage stats

## ğŸš€ Setup

```bash
# 1. Install packages
pip install -r requirements.txt

# 2. Download spaCy models
python -m spacy download xx_ent_wiki_sm
python -m spacy download en_core_web_sm

# 3. Test
python test_local_llm.py

# 4. Run UI
streamlit run app/ui/streamlit_app.py
```

## ğŸ› Troubleshooting

### OpenAI Unavailable
- âœ… Check internet connectivity
- âœ… Check your OpenAI API key
- âœ… Use Auto mode (falls back to Local)

### Local LLM Not Loading
- âœ… Ensure sufficient RAM (8GB+)
- âœ… Use a smaller model (DialoGPT-small)
- âœ… Disable GPU usage (USE_GPU=False in config.py)

### Performance Issues
- âœ… Use a smaller model for Local mode
- âœ… Use GPU if available
- âœ… Prefer Auto mode for best balance

## ğŸ’¡ Tips

1. First run: Start with Auto mode
2. Offline: Use Local Only mode
3. Best quality: Use OpenAI Only
4. Security: Use Local Only
5. Performance: Use Auto mode

## ğŸ“ˆ Performance Comparison

| Mode | Speed | Quality | Security | Cost |
|------|-------|---------|----------|------|
| Auto | â­â­â­ | â­â­â­ | â­â­â­ | â­â­ |
| OpenAI | â­â­â­ | â­â­â­ | â­â­ | â­ |
| Local | â­â­ | â­â­ | â­â­â­ | â­â­â­ |
